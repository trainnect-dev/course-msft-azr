{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, this is a comprehensive example lab that will guide you through building an end-to-end readmission prediction pipeline, consolidating many of the concepts and techniques from previous labs.\n",
        "\n",
        "---\n",
        "## ðŸ“˜ Section 12: Labs and Exercises\n",
        "\n",
        "### ðŸ§ª Lab 12.2: Example Lab: Build an End-to-End Readmission Prediction Pipeline\n",
        "\n",
        "**Objective:** To design and implement a complete end-to-end pipeline in Microsoft Fabric for predicting patient readmissions. This includes data ingestion (conceptual), data transformation, feature engineering, model training with MLflow, batch scoring, and creating a Power BI dashboard for visualization.\n",
        "\n",
        "**Scenario:** Valley General Hospital wants a robust system to identify patients at high risk of readmission within 30 days of discharge. This pipeline will provide actionable insights to care teams to implement interventions.\n",
        "\n",
        "**Assumed Prerequisites:**\n",
        "* A Microsoft Fabric enabled environment with an available capacity.\n",
        "* Permissions to create workspaces, Lakehouses, Notebooks, Data Factory items, and Power BI reports.\n",
        "* Understanding of concepts from Labs 2.9, 3.8, 4.8, 5.9, 6.8, and 7.8.\n",
        "\n",
        "---\n",
        "**Phase 1: Environment Setup & Data Ingestion (Recap & Assumptions)**\n",
        "\n",
        "* **Step 1.1: Create a New Workspace.**\n",
        "    * Name: `E2E_ReadmissionAnalytics_YourName`. Assign it to a Fabric capacity.\n",
        "* **Step 1.2: Create a New Lakehouse.**\n",
        "    * Within the new workspace, create a Lakehouse named `HealthClinicLH_E2E_YourName`.\n",
        "* **Step 1.3: Assumed Bronze Layer Data.**\n",
        "    * For this lab, we will assume that raw data has been ingested into the `Files` section of your Lakehouse (or into Bronze tables directly). We'll create sample CSV files and then load them into Spark DataFrames to simulate this.\n",
        "\n",
        "    * **Create `sample_patients_bronze.csv` locally:**\n",
        "        ```csv\n",
        "        PatientID,MRN,DateOfBirth,Gender,ZipCode,DeceasedDate\n",
        "        P001,MRN001,1965-03-15,Female,90210,\n",
        "        P002,MRN002,1972-07-22,Male,90010,\n",
        "        P003,MRN003,1950-11-02,Male,90211,2023-08-15\n",
        "        P004,MRN004,1985-01-30,Female,92101,\n",
        "        P005,MRN005,1990-05-10,Other,90030,\n",
        "        ```\n",
        "    * **Create `sample_encounters_bronze.csv` locally:**\n",
        "        ```csv\n",
        "        EncounterID,PatientID,AdmissionDate,DischargeDate,EncounterType,PrimaryDiagnosisCode,HospitalizationLengthDays,DischargeDisposition\n",
        "        E001,P001,2023-01-10T10:00:00,2023-01-15T14:30:00,Inpatient,I21.3,5,Home\n",
        "        E002,P002,2023-02-01T11:00:00,2023-02-05T12:00:00,Inpatient,J44.9,4,Skilled Nursing Facility\n",
        "        E003,P001,2023-02-10T09:00:00,2023-02-12T17:00:00,Inpatient,I21.3,2,Home\n",
        "        E004,P003,2023-03-01T15:00:00,2023-03-10T10:00:00,Inpatient,N18.6,9,Expired\n",
        "        E005,P002,2023-03-05T08:00:00,2023-03-08T18:00:00,Emergency,R07.4,3,Home\n",
        "        E006,P004,2023-04-10T16:00:00,2023-04-18T11:00:00,Inpatient,S06.9,8,Rehab Facility\n",
        "        E007,P001,2023-04-15T10:00:00,2023-04-20T14:30:00,Inpatient,I50.0,5,Home\n",
        "        E008,P005,2023-05-01T07:00:00,2023-05-03T10:00:00,Observation,Z03.89,2,Home\n",
        "        ```\n",
        "    * **Upload these CSV files:**\n",
        "        * In `HealthClinicLH_E2E_YourName`, navigate to `Files`.\n",
        "        * Create a folder named `bronze_layer_uploads`.\n",
        "        * Upload `sample_patients_bronze.csv` and `sample_encounters_bronze.csv` into this folder.\n",
        "\n",
        "---\n",
        "**Phase 2: Data Preparation and Transformation (Silver & Gold Layers)**\n",
        "\n",
        "* **Step 2.1: Create a New Notebook.**\n",
        "    * In your workspace, create a new Notebook named `Prepare_Readmission_Data_E2E_YourName`.\n",
        "    * Attach it to the `HealthClinicLH_E2E_YourName` Lakehouse.\n",
        "\n",
        "* **Step 2.2: Notebook Code for Data Prep (PySpark).**"
      ],
      "metadata": {
        "id": "khhF8V3XMkpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "    from pyspark.sql.window import Window\n",
        "    from pyspark.sql.types import DateType, TimestampType, IntegerType\n",
        "\n",
        "    lakehouse_name = \"HealthClinicLH_E2E_YourName\" # Ensure this matches your Lakehouse name\n",
        "\n",
        "    # --- Load Bronze Data ---\n",
        "    patients_bronze_df = spark.read.format(\"csv\") \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"inferSchema\", \"true\") \\\n",
        "        .load(f\"/lakehouses/{lakehouse_name}/Files/bronze_layer_uploads/sample_patients_bronze.csv\")\n",
        "\n",
        "    encounters_bronze_df = spark.read.format(\"csv\") \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"inferSchema\", \"true\") \\\n",
        "        .load(f\"/lakehouses/{lakehouse_name}/Files/bronze_layer_uploads/sample_encounters_bronze.csv\")\n",
        "\n",
        "    print(\"Bronze data loaded.\")\n",
        "    patients_bronze_df.show()\n",
        "    encounters_bronze_df.show()\n",
        "\n",
        "    # --- Create Dim_Patient_E2E (Gold) ---\n",
        "    dim_patient_df = patients_bronze_df \\\n",
        "        .withColumn(\"DateOfBirth\", F.to_date(F.col(\"DateOfBirth\"))) \\\n",
        "        .withColumn(\"DeceasedDate\", F.to_date(F.col(\"DeceasedDate\"))) \\\n",
        "        .withColumn(\"IsDeceased\", F.when(F.col(\"DeceasedDate\").isNotNull(), True).otherwise(False)) \\\n",
        "        .withColumn(\"Age\", F.floor(F.datediff(F.current_date(), F.col(\"DateOfBirth\")) / 365.25).cast(IntegerType())) \\\n",
        "        .withColumn(\"PatientKey\", F.md5(F.col(\"PatientID\"))) \\\n",
        "        .select(\"PatientKey\", \"PatientID\", \"MRN\", \"DateOfBirth\", \"Age\", \"Gender\", \"ZipCode\", \"IsDeceased\", \"DeceasedDate\")\n",
        "\n",
        "    dim_patient_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{lakehouse_name}.Dim_Patient_E2E\")\n",
        "    print(\"Dim_Patient_E2E created.\")\n",
        "    spark.read.table(f\"{lakehouse_name}.Dim_Patient_E2E\").show()\n",
        "\n",
        "    # --- Create Dim_Date_E2E (Gold) ---\n",
        "    # (Simplified Dim_Date for this lab)\n",
        "    date_range_df = spark.sql(\"SELECT sequence(to_date('2022-01-01'), to_date('2025-12-31'), interval 1 day) as date_array\")\n",
        "    dim_date_df = date_range_df.select(F.explode(F.col(\"date_array\")).alias(\"FullDate\")) \\\n",
        "        .withColumn(\"DateKey\", F.date_format(F.col(\"FullDate\"), \"yyyyMMdd\").cast(IntegerType())) \\\n",
        "        .withColumn(\"Year\", F.year(F.col(\"FullDate\"))) \\\n",
        "        .withColumn(\"Month\", F.month(F.col(\"FullDate\"))) \\\n",
        "        .withColumn(\"Day\", F.dayofmonth(F.col(\"FullDate\"))) \\\n",
        "        .withColumn(\"DayOfWeek\", F.dayofweek(F.col(\"FullDate\")))\n",
        "\n",
        "    dim_date_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{lakehouse_name}.Dim_Date_E2E\")\n",
        "    print(\"Dim_Date_E2E created.\")\n",
        "    spark.read.table(f\"{lakehouse_name}.Dim_Date_E2E\").show(5)\n",
        "\n",
        "    # --- Create Fact_Encounter_E2E (Gold) ---\n",
        "    # Convert string dates to timestamp/date\n",
        "    encounters_transformed_df = encounters_bronze_df \\\n",
        "        .withColumn(\"AdmissionDate\", F.to_timestamp(F.col(\"AdmissionDate\"))) \\\n",
        "        .withColumn(\"DischargeDate\", F.to_timestamp(F.col(\"DischargeDate\")))\n",
        "\n",
        "    # Calculate DaysToNextAdmission & PriorAdmissionsCount\n",
        "    window_patient_admission_asc = Window.partitionBy(\"PatientID\").orderBy(F.col(\"AdmissionDate\").asc())\n",
        "    window_patient_discharge_asc = Window.partitionBy(\"PatientID\").orderBy(F.col(\"DischargeDate\").asc())\n",
        "\n",
        "    fact_encounter_intermediate_df = encounters_transformed_df \\\n",
        "        .withColumn(\"NextAdmissionDate\", F.lead(F.col(\"AdmissionDate\"), 1).over(window_patient_admission_asc)) \\\n",
        "        .withColumn(\"PriorAdmissionsCount\", F.row_number().over(window_patient_admission_asc) - 1) \\\n",
        "        .withColumn(\"DaysToNextAdmission\", F.when(F.col(\"NextAdmissionDate\").isNotNull() & F.col(\"DischargeDate\").isNotNull(),\n",
        "                                                 F.datediff(F.col(\"NextAdmissionDate\"), F.col(\"DischargeDate\")))\n",
        "                      .otherwise(None))\n",
        "\n",
        "    # Join with Dim_Patient to get PatientKey, Dim_Date for AdmissionDateKey\n",
        "    fact_encounter_df = fact_encounter_intermediate_df \\\n",
        "        .join(dim_patient_df.select(\"PatientID\", \"PatientKey\"), \"PatientID\", \"inner\") \\\n",
        "        .join(dim_date_df.select(\"FullDate\", \"DateKey\").alias(\"dim_adm_date\"),\n",
        "              F.to_date(fact_encounter_intermediate_df.AdmissionDate) == F.col(\"dim_adm_date.FullDate\"), \"left\") \\\n",
        "        .select(\n",
        "            F.md5(F.col(\"EncounterID\")).alias(\"EncounterKey\"),\n",
        "            F.col(\"PatientKey\"),\n",
        "            F.col(\"dim_adm_date.DateKey\").alias(\"AdmissionDateKey\"),\n",
        "            F.to_date(F.col(\"AdmissionDate\")).alias(\"AdmissionDateOnly\"), # For easier use in ML\n",
        "            F.to_date(F.col(\"DischargeDate\")).alias(\"DischargeDateOnly\"), # For easier use in ML\n",
        "            F.col(\"EncounterType\"),\n",
        "            F.col(\"PrimaryDiagnosisCode\"),\n",
        "            F.col(\"HospitalizationLengthDays\").alias(\"LengthOfStayInDays\"), # Renaming for consistency\n",
        "            F.col(\"DischargeDisposition\"),\n",
        "            F.col(\"PriorAdmissionsCount\"),\n",
        "            F.col(\"DaysToNextAdmission\")\n",
        "        )\n",
        "\n",
        "    fact_encounter_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{lakehouse_name}.Fact_Encounter_E2E\")\n",
        "    print(\"Fact_Encounter_E2E created.\")\n",
        "    spark.read.table(f\"{lakehouse_name}.Fact_Encounter_E2E\").show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "NcApOMceMkpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Phase 3: Model Training and Experimentation**\n",
        "\n",
        "* **Step 3.1: Create a New Notebook.**\n",
        "    * Name: `Train_Readmission_Model_E2E_YourName`. Attach to `HealthClinicLH_E2E_YourName`.\n",
        "* **Step 3.2: Notebook Code for Model Training (PySpark & MLflow).**"
      ],
      "metadata": {
        "id": "y_KIeVZeMkpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "    from pyspark.sql import functions as F\n",
        "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer, StandardScaler\n",
        "    from pyspark.ml.classification import RandomForestClassifier # Using RF for this example\n",
        "    from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "    from pyspark.ml import Pipeline\n",
        "\n",
        "    lakehouse_name = \"HealthClinicLH_E2E_YourName\"\n",
        "    fact_table = f\"{lakehouse_name}.Fact_Encounter_E2E\"\n",
        "    dim_patient_table = f\"{lakehouse_name}.Dim_Patient_E2E\"\n",
        "\n",
        "    # --- Load Data for Modeling ---\n",
        "    fact_df = spark.read.table(fact_table)\n",
        "    patient_df = spark.read.table(dim_patient_table)\n",
        "\n",
        "    model_input_df = fact_df.join(patient_df, \"PatientKey\", \"inner\") \\\n",
        "        .select(\n",
        "            fact_df.LengthOfStayInDays,\n",
        "            fact_df.PrimaryDiagnosisCode,\n",
        "            fact_df.EncounterType,\n",
        "            fact_df.PriorAdmissionsCount,\n",
        "            fact_df.DaysToNextAdmission,\n",
        "            patient_df.Age,\n",
        "            patient_df.Gender,\n",
        "            patient_df.PatientKey # Keep for potential analysis, but not as a feature\n",
        "        ).na.drop(subset=[\"DaysToNextAdmission\", \"LengthOfStayInDays\", \"Age\", \"PriorAdmissionsCount\"]) # Drop if target or key numeric features are null\n",
        "\n",
        "    # --- Feature Engineering for Model ---\n",
        "    # Create Target Variable\n",
        "    model_input_df = model_input_df.withColumn(\n",
        "        \"label\", # Standard name for target in Spark ML\n",
        "        F.when((F.col(\"DaysToNextAdmission\") >= 0) & (F.col(\"DaysToNextAdmission\") <= 30), 1.0).otherwise(0.0)\n",
        "    )\n",
        "\n",
        "    # Handle categorical features\n",
        "    categorical_cols = [\"PrimaryDiagnosisCode\", \"EncounterType\", \"Gender\"]\n",
        "    indexers = [StringIndexer(inputCol=col, outputCol=col+\"_Index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "    encoders = [OneHotEncoder(inputCol=col+\"_Index\", outputCol=col+\"_Vec\") for col in categorical_cols]\n",
        "\n",
        "    # Handle numerical features (impute nulls if any before scaling)\n",
        "    numerical_cols = [\"LengthOfStayInDays\", \"PriorAdmissionsCount\", \"Age\"]\n",
        "    imputer_numerical = Imputer(inputCols=numerical_cols, outputCols=[c + \"_Imputed\" for c in numerical_cols], strategy=\"median\")\n",
        "\n",
        "    # Assemble features (numerical imputed + categorical one-hot encoded)\n",
        "    imputed_numerical_feature_cols = [c + \"_Imputed\" for c in numerical_cols]\n",
        "    encoded_categorical_feature_cols = [col+\"_Vec\" for col in categorical_cols]\n",
        "    assembler_inputs = imputed_numerical_feature_cols + encoded_categorical_feature_cols\n",
        "    vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"assembledFeatures\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler(inputCol=\"assembledFeatures\", outputCol=\"features\") # 'features' is standard name\n",
        "\n",
        "    # Split data\n",
        "    train_df, test_df = model_input_df.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "    # --- Model Training with MLflow ---\n",
        "    with mlflow.start_run(run_name=\"RandomForest_Readmission_E2E\") as run:\n",
        "        mlflow.log_param(\"data_source_fact\", fact_table)\n",
        "        mlflow.log_param(\"data_source_dim_patient\", dim_patient_table)\n",
        "        mlflow.log_param(\"train_test_split_ratio\", \"0.8/0.2\")\n",
        "\n",
        "        # Define Random Forest model\n",
        "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxDepth=10, seed=123)\n",
        "        mlflow.log_param(\"numTrees\", 100)\n",
        "        mlflow.log_param(\"maxDepth\", 10)\n",
        "\n",
        "        # Create pipeline\n",
        "        pipeline_stages = [imputer_numerical] + indexers + encoders + [vector_assembler, scaler, rf]\n",
        "        pipeline_rf = Pipeline(stages=pipeline_stages)\n",
        "\n",
        "        print(\"Training Random Forest model...\")\n",
        "        rf_model = pipeline_rf.fit(train_df)\n",
        "        print(\"Model training complete.\")\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = rf_model.transform(test_df)\n",
        "\n",
        "        # Evaluate model\n",
        "        evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "        evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\") # F1 for positive class (1.0)\n",
        "\n",
        "        auc_score = evaluator_auc.evaluate(predictions)\n",
        "        # For F1 of positive class, need to filter or use appropriate evaluator setup\n",
        "        f1_score = evaluator_f1.setMetricLabel(1.0).evaluate(predictions.filter(F.col(\"label\") == 1.0)) if predictions.filter(F.col(\"label\") == 1.0).count() > 0 else 0.0\n",
        "\n",
        "\n",
        "        print(f\"Test AUC: {auc_score}\")\n",
        "        print(f\"Test F1-Score (for class 1): {f1_score}\")\n",
        "\n",
        "        mlflow.log_metric(\"AUC\", auc_score)\n",
        "        mlflow.log_metric(\"F1_Class1\", f1_score)\n",
        "\n",
        "        # Log model\n",
        "        mlflow.spark.log_model(\n",
        "            spark_model=rf_model,\n",
        "            artifact_path=\"readmission_rf_pipeline_model\",\n",
        "            registered_model_name=\"ReadmissionRiskModelE2E\" # This will register the model\n",
        "        )\n",
        "        print(\"Random Forest model and metrics logged to MLflow, and model registered.\")\n",
        "\n",
        "        run_id_rf = run.info.run_id\n",
        "        mlflow.end_run()\n",
        "\n",
        "    print(f\"MLflow Run ID for Random Forest E2E: {run_id_rf}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "JQaV2U3eMkpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Phase 4: Batch Scoring of Upcoming Appointments**\n",
        "\n",
        "* **Step 4.1: Create a New Notebook.**\n",
        "    * Name: `Score_Upcoming_Appointments_E2E_YourName`. Attach to `HealthClinicLH_E2E_YourName`.\n",
        "* **Step 4.2: Notebook Code for Batch Scoring.**"
      ],
      "metadata": {
        "id": "zpAFegLOMkpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "    from pyspark.sql import functions as F\n",
        "\n",
        "    lakehouse_name = \"HealthClinicLH_E2E_YourName\"\n",
        "    registered_model_name = \"ReadmissionRiskModelE2E\"\n",
        "\n",
        "    # --- Simulate Upcoming Appointments Data ---\n",
        "    # In a real scenario, this would be new data. Here, we'll use a subset of Fact_Encounter_E2E\n",
        "    # and pretend it's upcoming (remove DaysToNextAdmission and label).\n",
        "    # Or, create a new sample CSV similar to encounters_bronze but for future dates.\n",
        "\n",
        "    # For simplicity, let's just re-load part of Fact_Encounter_E2E and necessary Dim_Patient columns\n",
        "    # and select rows that were NOT readmitted (label=0) to act as our \"new, unscored\" data.\n",
        "    fact_df_all = spark.read.table(f\"{lakehouse_name}.Fact_Encounter_E2E\")\n",
        "    patient_df_all = spark.read.table(f\"{lakehouse_name}.Dim_Patient_E2E\")\n",
        "\n",
        "    upcoming_appointments_raw_df = fact_df_all.join(patient_df_all, \"PatientKey\", \"inner\") \\\n",
        "        .select(\n",
        "            fact_df_all.EncounterKey, # For identifying the appointment\n",
        "            fact_df_all.PatientKey,\n",
        "            fact_df_all.LengthOfStayInDays,\n",
        "            fact_df_all.PrimaryDiagnosisCode,\n",
        "            fact_df_all.EncounterType,\n",
        "            fact_df_all.PriorAdmissionsCount,\n",
        "            patient_df_all.Age,\n",
        "            patient_df_all.Gender\n",
        "        ).limit(2) # Take a small sample to simulate \"new\" data for scoring\n",
        "\n",
        "    if upcoming_appointments_raw_df.count() == 0:\n",
        "        print(\"No data in upcoming_appointments_raw_df to score. Exiting scoring.\")\n",
        "    else:\n",
        "        print(\"Simulated upcoming appointments data:\")\n",
        "        upcoming_appointments_raw_df.show()\n",
        "\n",
        "        # --- Load Registered Model ---\n",
        "        # Load the latest version of the registered model\n",
        "        # The model URI format is \"models:/<model_name>/<version>\" or \"models:/<model_name>/latest\"\n",
        "        try:\n",
        "            loaded_model = mlflow.spark.load_model(f\"models:/{registered_model_name}/latest\")\n",
        "            print(f\"Successfully loaded model '{registered_model_name}/latest'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}. Make sure the model '{registered_model_name}' is registered.\")\n",
        "            dbutils.notebook.exit(\"Model loading failed.\")\n",
        "\n",
        "\n",
        "        # --- Make Predictions ---\n",
        "        # The loaded_model is the entire pipeline, so it will handle preprocessing.\n",
        "        predictions_df = loaded_model.transform(upcoming_appointments_raw_df)\n",
        "\n",
        "        # Select relevant output columns\n",
        "        # The probability for class 1 (readmission) is usually the second element in the 'probability' vector\n",
        "        scored_appointments_df = predictions_df.select(\n",
        "            F.col(\"EncounterKey\"),\n",
        "            F.col(\"PatientKey\"),\n",
        "            F.col(\"prediction\").alias(\"PredictedReadmissionLabel\"), # 0 or 1\n",
        "            F.element_at(F.col(\"probability\"), 2).alias(\"ReadmissionProbability\") # Probability of class 1\n",
        "        )\n",
        "\n",
        "        print(\"Scored appointments:\")\n",
        "        scored_appointments_df.show(truncate=False)\n",
        "\n",
        "        # --- Save Predictions ---\n",
        "        scored_appointments_df.write.format(\"delta\").mode(\"overwrite\") \\\n",
        "            .saveAsTable(f\"{lakehouse_name}.Gold_Upcoming_ReadmissionPredictions_E2E\")\n",
        "        print(f\"Predictions saved to Gold_Upcoming_ReadmissionPredictions_E2E\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "YFVhIpo3Mkpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Phase 5: Reporting and Visualization**\n",
        "\n",
        "* **Step 5.1: Create a New Power BI Report.**\n",
        "    * In your Fabric workspace, create a new Power BI report named `ReadmissionRiskDashboard_E2E`.\n",
        "* **Step 5.2: Connect to Data.**\n",
        "    * Connect to your `HealthClinicLH_E2E_YourName` Lakehouse SQL Endpoint.\n",
        "    * Load tables:\n",
        "        * `Gold_Upcoming_ReadmissionPredictions_E2E`\n",
        "        * `Dim_Patient_E2E`\n",
        "        * `Fact_Encounter_E2E` (for historical context or joining to get more appointment details if needed for the predictions)\n",
        "        * `Dim_Date_E2E`\n",
        "* **Step 5.3: Create Relationships in Power BI Model.**\n",
        "    * `Gold_Upcoming_ReadmissionPredictions_E2E[PatientKey]` to `Dim_Patient_E2E[PatientKey]`\n",
        "    * (If joining predictions back to original encounter details) `Gold_Upcoming_ReadmissionPredictions_E2E[EncounterKey]` to `Fact_Encounter_E2E[EncounterKey]`\n",
        "* **Step 5.4: Design Dashboard (Conceptual).**\n",
        "    * **KPIs:** Overall Predicted Readmission Rate (from `ReadmissionProbability`), Number of High-Risk Patients.\n",
        "    * **Table/List:** Upcoming appointments, sorted by `ReadmissionProbability` (descending), showing Patient Name (from `Dim_Patient_E2E`), predicted probability, and original appointment details (from `Fact_Encounter_E2E` if joined).\n",
        "    * **Charts:**\n",
        "        * Distribution of `ReadmissionProbability` scores.\n",
        "        * Count of high-risk patients by clinic (if `AssignedClinicID` is in `Dim_Patient_E2E` and joined).\n",
        "        * Average `LengthOfStayInDays` or `PriorAdmissionsCount` for high-risk vs. low-risk predicted patients.\n",
        "    * **Slicers:** By date (for upcoming appointments), by risk level, by clinic.\n",
        "\n",
        "---\n",
        "**Phase 6: Governance and Security (Recap & Considerations)**\n",
        "\n",
        "* **Microsoft Purview:**\n",
        "    * Ensure the `E2E_ReadmissionAnalytics_YourName` workspace is scanned.\n",
        "    * Classify `Dim_Patient_E2E` and `Fact_Encounter_E2E` with appropriate sensitivity labels (e.g., \"Confidential - PHI\").\n",
        "    * Review lineage from Bronze data through Gold predictions to the Power BI report.\n",
        "* **Access Control:**\n",
        "    * Define workspace roles (Admin, Member, Contributor, Viewer) for different team members.\n",
        "    * Share the `ReadmissionRiskDashboard_E2E` Power BI report with \"View\" access to care teams/schedulers.\n",
        "    * Implement Row-Level Security (RLS) on the Power BI dataset so that users only see predictions for patients within their authorized scope (e.g., their clinic/department).\n",
        "* **Auditing:** Use Fabric audit logs to monitor access to sensitive data and the dashboard.\n",
        "* **MLOps (MLflow):** The use of MLflow for model versioning, registration, and tracking metrics is a core part of MLOps and governance for the ML solution.\n",
        "\n",
        "This end-to-end lab provides a framework. Each phase can be expanded with more detailed data, complex feature engineering, more sophisticated model tuning, and more elaborate Power BI visualizations based on specific organizational needs.\n",
        "\n",
        "---\n",
        "*(Continuing to Lab 12.3 - Challenge Lab)*"
      ],
      "metadata": {
        "id": "7TCkA17hMkpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples\">https://github.com/GoogleCloudPlatform/vertex-ai-samples</a> subject to Apache - 2.0</li>\n",
        "  <li><a href=\"https://github.com/sara-hammami/chirn-prediction-with-Pyspark-\">https://github.com/sara-hammami/chirn-prediction-with-Pyspark-</a></li>\n",
        "  <li><a href=\"https://github.com/kvsmadhulika/Madhulika_koduru\">https://github.com/kvsmadhulika/Madhulika_koduru</a></li>\n",
        "  <li><a href=\"https://github.com/meznah1995/spamemail\">https://github.com/meznah1995/spamemail</a></li>\n",
        "  <li><a href=\"https://github.com/currencyfxjle/PySpark_Models_Evaluation\">https://github.com/currencyfxjle/PySpark_Models_Evaluation</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "ZX8U8yglMkpy"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}