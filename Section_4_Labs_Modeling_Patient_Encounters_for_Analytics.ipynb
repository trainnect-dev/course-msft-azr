{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQmjiw1q_ePf"
      },
      "source": [
        "## ðŸ“˜ Section 4: Data Modeling and Transformation\n",
        "\n",
        "### ðŸ§ª Lab 4.8: Modeling Patient Encounters for Analytics\n",
        "\n",
        "**Objective:** Design and conceptually implement the transformation of raw patient encounter data (from Bronze) into a structured Silver layer table and then into a simple Gold layer dimensional model.\n",
        "\n",
        "**Assumed Bronze Layer Data (from previous ingestion labs):**\n",
        "* `Bronze_HL7_Encounters_Raw` (parsed from HL7 ADT messages in Lakehouse: `HealthDataLH_YourName`)\n",
        "* `Bronze_FHIR_Encounters_Parsed` (parsed JSON from FHIR API in Lakehouse: `HealthDataLH_YourName`)\n",
        "* A `Bronze_Patients_Raw` table. For this lab, let's define its schema and provide sample data creation.\n",
        "\n",
        "**Creating Sample `Bronze_Patients_Raw` Data (PySpark Notebook Cell):**\n",
        "This would typically be ingested from an EHR dump or FHIR Patient resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41H4E8eU_ePh"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "import datetime\n",
        "\n",
        "lakehouse_name = \"HealthDataLH_YourName\" # Replace YourName\n",
        "bronze_patients_table = f\"{lakehouse_name}.Bronze_Patients_Raw\"\n",
        "\n",
        "# Sample patient data (could come from various sources)\n",
        "patient_data = [\n",
        "    Row(PatientSourceID=\"PATID12345\", MRN=\"MRN001\", SSN_Token=\"TOKEN_SSN1\", FirstName=\"John\", LastName=\"Doe\", DateOfBirth=datetime.date(1980, 1, 1), Gender=\"M\", Race=\"WH\", Ethnicity=\"N\", ZipCode=\"90210\", SourceSystem=\"EHR_SystemA\", IngestionTimestamp=datetime.datetime.now()),\n",
        "    Row(PatientSourceID=\"PATID67890\", MRN=\"MRN002\", SSN_Token=\"TOKEN_SSN2\", FirstName=\"Jane\", LastName=\"Roe\", DateOfBirth=datetime.date(1975, 3, 15), Gender=\"F\", Race=\"AS\", Ethnicity=\"N\", ZipCode=\"90211\", SourceSystem=\"EHR_SystemA\", IngestionTimestamp=datetime.datetime.now()),\n",
        "    Row(PatientSourceID=\"PATFHIR001\", MRN=\"MRN003\", SSN_Token=\"TOKEN_SSN3\", FirstName=\"Walter\", LastName=\"White\", DateOfBirth=datetime.date(1965, 9, 7), Gender=\"M\", Race=\"WH\", Ethnicity=\"N\", ZipCode=\"87101\", SourceSystem=\"FHIR_API\", IngestionTimestamp=datetime.datetime.now()),\n",
        "    Row(PatientSourceID=\"EXTRALONGPATIENTID004\", MRN=\"MRN004\", SSN_Token=\"TOKEN_SSN4\", FirstName=\"Sarah\", LastName=\"Connor\", DateOfBirth=datetime.date(1985, 5, 10), Gender=\"F\", Race=\"WH\", Ethnicity=\"H\", ZipCode=\"90001\", SourceSystem=\"EHR_SystemB\", IngestionTimestamp=datetime.datetime.now()),\n",
        "    Row(PatientSourceID=\"PATID12345\", MRN=\"MRN001\", SSN_Token=\"TOKEN_SSN1\", FirstName=\"John\", LastName=\"Doe\", DateOfBirth=datetime.date(1980, 1, 1), Gender=\"M\", Race=\"WH\", Ethnicity=\"N\", ZipCode=\"90210\", SourceSystem=\"EHR_SystemA_OldRecord\", IngestionTimestamp=datetime.datetime.now()-datetime.timedelta(days=10)) # Duplicate for testing dedupe\n",
        "]\n",
        "\n",
        "patients_df = spark.createDataFrame(patient_data)\n",
        "\n",
        "patients_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_patients_table)\n",
        "print(f\"Sample data written to {bronze_patients_table}\")\n",
        "spark.read.table(bronze_patients_table).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaEVrMIx_ePi"
      },
      "source": [
        "**Assumed Silver Layer Data (from Lab 3.8):**\n",
        "* `Silver_Unified_Encounters` (in Lakehouse: `HealthDataLH_YourName`)\n",
        "\n",
        "**Lab Tasks (Conceptual Steps & Code):**\n",
        "\n",
        "**1. (Re-run/Verify) Create `Silver_Unified_Encounters` Table:**\n",
        "    * **Tool:** Spark Notebook (from Lab 3.8).\n",
        "    * **Logic:** Ensure the PySpark code from Lab 3.8, Task 3 (Conform and Integrate into a Silver Layer Table) has been run and the `Silver_Unified_Encounters` table exists in your `HealthDataLH_YourName` Lakehouse. This table should have columns like `SourceSystemEncounterID`, `PatientIdentifier`, `AdmissionDateTime`, `DischargeDateTime`, `EncounterType`, `AttendingProviderIdentifier`, `PrimaryDiagnosisCode`, `SourceSystem`, `SilverLoadTimestamp`, `EncounterSK`.\n",
        "\n",
        "---\n",
        "**2. Create `Dim_Patient` (Dimension Table - Gold Layer):**\n",
        "    * **Tool:** Spark Notebook.\n",
        "    * **Notebook Name:** `Create_Dim_Patient_Gold_YourName`\n",
        "    * **Logic (PySpark Code):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xudDBZ8k_ePj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, lit, current_timestamp, expr, monotonically_increasing_id, md5, concat_ws, year, month, dayofmonth, datediff, current_date, coalesce\n",
        "import datetime\n",
        "\n",
        "lakehouse_name = \"HealthDataLH_YourName\" # Replace YourName\n",
        "bronze_patients_table = f\"{lakehouse_name}.Bronze_Patients_Raw\"\n",
        "dim_patient_table = f\"{lakehouse_name}.Dim_Patient\" # Gold Layer Table\n",
        "\n",
        "# Read from Bronze_Patients_Raw table\n",
        "try:\n",
        "    raw_patients_df = spark.read.table(bronze_patients_table)\n",
        "except Exception as e:\n",
        "    print(f\"Error reading table {bronze_patients_table}: {e}\")\n",
        "    dbutils.notebook.exit(f\"Failed to read {bronze_patients_table}\")\n",
        "\n",
        "# --- De-duplicate patient records ---\n",
        "# Assuming MRN is a good candidate for a natural key for de-duplication\n",
        "# Keep the record with the latest ingestion timestamp in case of duplicates\n",
        "window_spec_patient = spark.catalog.Window.partitionBy(\"MRN\").orderBy(col(\"IngestionTimestamp\").desc())\n",
        "\n",
        "deduplicated_patients_df = raw_patients_df.withColumn(\"row_num\", expr(\"row_number() OVER (PARTITION BY MRN ORDER BY IngestionTimestamp DESC)\")) \\\n",
        "    .filter(col(\"row_num\") == 1) \\\n",
        "    .drop(\"row_num\")\n",
        "\n",
        "# --- Select and Transform for Dimension Table ---\n",
        "dim_patient_df = deduplicated_patients_df.select(\n",
        "    col(\"MRN\").alias(\"PatientNaturalKey\"), # Natural Key from source system (e.g., MRN)\n",
        "    col(\"PatientSourceID\").alias(\"SourcePatientID\"),\n",
        "    col(\"FirstName\"),\n",
        "    col(\"LastName\"),\n",
        "    col(\"DateOfBirth\").cast(\"date\"),\n",
        "    col(\"Gender\"), # Consider mapping to standard codes if necessary\n",
        "    col(\"Race\"),   # Consider mapping\n",
        "    col(\"Ethnicity\"), # Consider mapping\n",
        "    col(\"ZipCode\"),\n",
        "    col(\"SourceSystem\").alias(\"PatientSourceSystem\")\n",
        ").withColumn(\n",
        "    \"PatientKey\", md5(col(\"PatientNaturalKey\")) # Surrogate Key (MD5 hash of MRN for simplicity)\n",
        "    # For a robust SK, an incrementing ID or a more sophisticated hash might be used.\n",
        "    # monotonically_increasing_id() can be used but has caveats in distributed environments for strict sequential IDs.\n",
        ").withColumn(\n",
        "    \"FullName\", expr(\"concat(FirstName, ' ', LastName)\")\n",
        ").withColumn(\n",
        "    \"Age\", expr(\"floor(datediff(current_date(), DateOfBirth) / 365.25)\").cast(\"int\") # Calculated Age\n",
        ").withColumn(\n",
        "    \"GoldLoadTimestamp\", current_timestamp()\n",
        ").withColumn(\n",
        "    \"EffectiveStartDate\", lit(datetime.date(1900, 1, 1)).cast(\"date\") # For Type 2 SCD, default\n",
        ").withColumn(\n",
        "    \"EffectiveEndDate\", lit(None).cast(\"date\") # For Type 2 SCD, default\n",
        ").withColumn(\n",
        "    \"IsCurrent\", lit(True).cast(\"boolean\") # For Type 2 SCD\n",
        ")\n",
        "\n",
        "# Reorder columns for clarity in the dimension table\n",
        "final_dim_patient_df = dim_patient_df.select(\n",
        "    \"PatientKey\", \"PatientNaturalKey\", \"SourcePatientID\", \"FirstName\", \"LastName\", \"FullName\", \"DateOfBirth\", \"Age\", \"Gender\",\n",
        "    \"Race\", \"Ethnicity\", \"ZipCode\", \"PatientSourceSystem\",\n",
        "    \"EffectiveStartDate\", \"EffectiveEndDate\", \"IsCurrent\", \"GoldLoadTimestamp\"\n",
        ")\n",
        "\n",
        "# Write to Gold Layer Dim_Patient table\n",
        "final_dim_patient_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(dim_patient_table) # Use \"overwrite\" for this lab; \"merge\" for SCD Type 2 updates\n",
        "print(f\"Successfully created/updated {dim_patient_table}\")\n",
        "final_dim_patient_df.show(truncate=False) for clarity in the dimension table\n",
        "final_dim_patient_df = dim_patient_df.select(\n",
        "    \"PatientKey\", \"PatientNaturalKey\", \"SourcePatientID\", \"FirstName\", \"LastName\", \"FullName\", \"DateOfBirth\", \"Age\", \"Gender\",\n",
        "    \"Race\", \"Ethnicity\", \"ZipCode\", \"PatientSourceSystem\",\n",
        "    \"EffectiveStartDate\", \"EffectiveEndDate\", \"IsCurrent\", \"GoldLoadTimestamp\"\n",
        ")\n",
        "\n",
        "# Write to Gold Layer Dim_Patient table\n",
        "final_dim_patient_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(dim_patient_table) # Use \"overwrite\" for this lab; \"merge\" for SCD Type 2 updates\n",
        "print(f\"Successfully created/updated {dim_patient_table}\")\n",
        "final_dim_patient_df.show(truncate=False) for clarity in the dimension table\n",
        "final_dim_patient_df = dim_patient_df.select(\n",
        "    \"PatientKey\", \"PatientNaturalKey\", \"SourcePatientID\", \"FirstName\", \"LastName\", \"FullName\", \"DateOfBirth\", \"Age\", \"Gender\",\n",
        "    \"Race\", \"Ethnicity\", \"ZipCode\", \"PatientSourceSystem\",\n",
        "    \"EffectiveStartDate\", \"EffectiveEndDate\", \"IsCurrent\", \"GoldLoadTimestamp\"\n",
        ")\n",
        "\n",
        "# Write to Gold Layer Dim_Patient table\n",
        "final_dim_patient_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(dim_patient_table) # Use \"overwrite\" for this lab; \"merge\" for SCD Type 2 updates\n",
        "print(f\"Successfully created/updated {dim_patient_table}\")\n",
        "final_dim_patient_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIDCDzsL_ePk"
      },
      "source": [
        "---\n",
        "**3. Create `Fact_Encounter` (Fact Table - Gold Layer):**\n",
        "    * **Tool:** Spark Notebook.\n",
        "    * **Notebook Name:** `Create_Fact_Encounter_Gold_YourName`\n",
        "    * **Logic (PySpark Code):**\n",
        "        *(This assumes `Dim_Patient` has been created. For `Dim_Date` and `Dim_Provider`, we will create simplified placeholder versions or skip them for this lab's core focus on `Fact_Encounter` creation. In a real scenario, these would be properly built dimensions.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS-nNRaa_ePk"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, lit, current_timestamp, expr, to_date, datediff, year, month, dayofmonth, coalesce, when, explode\n",
        "import datetime\n",
        "\n",
        "lakehouse_name = \"HealthDataLH_YourName\" # Replace YourName\n",
        "silver_unified_encounters_table = f\"{lakehouse_name}.Silver_Unified_Encounters\"\n",
        "dim_patient_table = f\"{lakehouse_name}.Dim_Patient\"\n",
        "fact_encounter_table = f\"{lakehouse_name}.Fact_Encounter\" # Gold Layer Table\n",
        "\n",
        "# --- (Optional) Create a simple Dim_Date if not existing for lookup ---\n",
        "# This is a very basic Dim_Date for joining. A full Dim_Date is more comprehensive.\n",
        "dim_date_table = f\"{lakehouse_name}.Dim_Date\"\n",
        "try:\n",
        "    spark.read.table(dim_date_table).limit(1).collect()\n",
        "    print(f\"{dim_date_table} already exists.\")\n",
        "except:\n",
        "    print(f\"Creating simplified {dim_date_table}...\")\n",
        "    date_range_df = spark.sql(\"SELECT sequence(to_date('2020-01-01'), to_date('2030-12-31'), interval 1 day) as date_array\")\n",
        "    dates_df = date_range_df.select(explode(col(\"date_array\")).alias(\"FullDate\"))\n",
        "    simple_dim_date_df = dates_df.select(\n",
        "        expr(\"replace(cast(FullDate as string), '-', '')\").cast(\"int\").alias(\"DateKey\"), # Format YYYYMMDD\n",
        "        col(\"FullDate\").cast(\"date\"),\n",
        "        year(col(\"FullDate\")).alias(\"Year\"),\n",
        "        month(col(\"FullDate\")).alias(\"Month\"),\n",
        "        dayofmonth(col(\"FullDate\")).alias(\"Day\")\n",
        "    )\n",
        "    simple_dim_date_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(dim_date_table)\n",
        "    print(f\"Simplified {dim_date_table} created.\")\n",
        "\n",
        "# Read source tables\n",
        "try:\n",
        "    encounters_df = spark.read.table(silver_unified_encounters_table)\n",
        "    patients_dim_df = spark.read.table(dim_patient_table).select(\"PatientKey\", \"PatientNaturalKey\") # Only need keys for join\n",
        "    date_dim_df = spark.read.table(dim_date_table).select(\"DateKey\", \"FullDate\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading source tables for Fact_Encounter: {e}\")\n",
        "    dbutils.notebook.exit(\"Failed to read source tables.\")\n",
        "\n",
        "# Join Encounters with Dim_Patient to get PatientKey\n",
        "# Assuming encounters_df.PatientIdentifier corresponds to patients_dim_df.PatientNaturalKey (e.g., MRN)\n",
        "fact_df_intermediate = encounters_df.join(\n",
        "    patients_dim_df,\n",
        "    encounters_df.PatientIdentifier == patients_dim_df.PatientNaturalKey,\n",
        "    \"left_outer\"\n",
        ").select(\n",
        "    encounters_df[\"*\"], # Select all columns from encounters_df\n",
        "    patients_dim_df[\"PatientKey\"]\n",
        ")\n",
        "\n",
        "# Join with Dim_Date for AdmissionDateKey\n",
        "fact_df_intermediate = fact_df_intermediate.join(\n",
        "    date_dim_df.alias(\"dim_admission_date\"),\n",
        "    to_date(fact_df_intermediate.AdmissionDateTime) == col(\"dim_admission_date.FullDate\"),\n",
        "    \"left_outer\"\n",
        ").select(\n",
        "    fact_df_intermediate[\"*\"],\n",
        "    col(\"dim_admission_date.DateKey\").alias(\"AdmissionDateKey\")\n",
        ")\n",
        "\n",
        "# Join with Dim_Date for DischargeDateKey\n",
        "fact_df_intermediate = fact_df_intermediate.join(\n",
        "    date_dim_df.alias(\"dim_discharge_date\"),\n",
        "    to_date(fact_df_intermediate.DischargeDateTime) == col(\"dim_discharge_date.FullDate\"),\n",
        "    \"left_outer\"\n",
        ").select(\n",
        "    fact_df_intermediate[\"*\"],\n",
        "    col(\"dim_discharge_date.DateKey\").alias(\"DischargeDateKey\")\n",
        ")\n",
        "\n",
        "# --- Select Measures and Foreign Keys for Fact Table ---\n",
        "fact_encounter_df = fact_df_intermediate.withColumn(\n",
        "    \"LengthOfStayInDays\",\n",
        "    when(col(\"DischargeDateTime\").isNotNull() & col(\"AdmissionDateTime\").isNotNull(),\n",
        "         datediff(to_date(col(\"DischargeDateTime\")), to_date(col(\"AdmissionDateTime\")))\n",
        "    ).otherwise(None).cast(\"int\")\n",
        ").select(\n",
        "    col(\"EncounterSK\").alias(\"EncounterKey\"), # Surrogate key from Silver layer or newly generated\n",
        "    col(\"PatientKey\"), # Foreign Key from Dim_Patient\n",
        "    col(\"AdmissionDateKey\"), # Foreign Key from Dim_Date\n",
        "    col(\"DischargeDateKey\"), # Foreign Key from Dim_Date\n",
        "    # Add ProviderKey if Dim_Provider exists and is joined\n",
        "    # col(\"ProviderKey\"),\n",
        "    col(\"SourceSystemEncounterID\").alias(\"EncounterNaturalKey\"), # Natural key from source\n",
        "    col(\"EncounterType\"),\n",
        "    col(\"PrimaryDiagnosisCode\"),\n",
        "    col(\"AttendingProviderIdentifier\"), # Could be a degenerate dimension or FK to Dim_Provider\n",
        "    col(\"LengthOfStayInDays\"),\n",
        "    # Add other measures like TotalCharges if available\n",
        "    # lit(1).alias(\"EncounterCount\"), # Useful measure\n",
        "    col(\"SourceSystem\"),\n",
        "    current_timestamp().alias(\"GoldLoadTimestamp\")\n",
        ").filter(col(\"PatientKey\").isNotNull()) # Only load encounters that have a matching patient in Dim_Patient\n",
        "\n",
        "# Write to Gold Layer Fact_Encounter table\n",
        "fact_encounter_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(fact_encounter_table)\n",
        "print(f\"Successfully created/updated {fact_encounter_table}\")\n",
        "fact_encounter_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B91bmcsy_ePl"
      },
      "source": [
        "---\n",
        "**4. (Optional) Create `Patient_Visit_Analytics_View` (Gold Layer View):**\n",
        "    * **Tool:** SQL Endpoint of the `HealthDataLH_YourName` Lakehouse or a Fabric Warehouse.\n",
        "    * **Logic (SQL Code):**\n",
        "        Open a new SQL query window connected to your Lakehouse SQL endpoint.\n",
        "\n",
        "        ```sql\n",
        "        -- Ensure you are in the context of your Lakehouse, e.g., USE HealthDataLH_YourName;\n",
        "        -- Or fully qualify table names if needed: HealthDataLH_YourName.dbo.Fact_Encounter\n",
        "\n",
        "        CREATE OR ALTER VIEW Patient_Visit_Analytics_View AS\n",
        "        SELECT\n",
        "            p.PatientNaturalKey AS PatientMRN,\n",
        "            p.FullName AS PatientFullName,\n",
        "            p.DateOfBirth AS PatientDateOfBirth,\n",
        "            p.Age AS PatientAge,\n",
        "            p.Gender AS PatientGender,\n",
        "            fe.EncounterNaturalKey,\n",
        "            fe.EncounterType,\n",
        "            fe.PrimaryDiagnosisCode,\n",
        "            adm_dt.FullDate AS AdmissionDate,\n",
        "            dis_dt.FullDate AS DischargeDate,\n",
        "            fe.LengthOfStayInDays,\n",
        "            fe.AttendingProviderIdentifier,\n",
        "            fe.SourceSystem AS EncounterSourceSystem\n",
        "            -- Add more fields from dimensions (Dim_Provider, Dim_Diagnosis) as needed\n",
        "        FROM\n",
        "            HealthDataLH_YourName.dbo.Fact_Encounter fe\n",
        "        JOIN\n",
        "            HealthDataLH_YourName.dbo.Dim_Patient p ON fe.PatientKey = p.PatientKey\n",
        "        LEFT JOIN\n",
        "            HealthDataLH_YourName.dbo.Dim_Date adm_dt ON fe.AdmissionDateKey = adm_dt.DateKey\n",
        "        LEFT JOIN\n",
        "            HealthDataLH_YourName.dbo.Dim_Date dis_dt ON fe.DischargeDateKey = dis_dt.DateKey\n",
        "        -- LEFT JOIN\n",
        "        --     HealthDataLH_YourName.dbo.Dim_Provider dp ON fe.ProviderKey = dp.ProviderKey -- If Dim_Provider exists\n",
        "        ;\n",
        "\n",
        "        -- Test the view\n",
        "        SELECT * FROM Patient_Visit_Analytics_View LIMIT 10;\n",
        "        ```\n",
        "\n",
        "---\n",
        "**Discussion Questions:**\n",
        "1.  **In `Dim_Patient`, why is it generally better to use a system-generated `PatientKey` (surrogate key) as the primary key instead of using the `PatientIdentifier` (natural key from the source system) directly?**\n",
        "    * **Stability:** Natural keys (like MRN) can sometimes change, be reassigned, or have errors in the source system. Surrogate keys are controlled within the data warehouse and remain stable, protecting the fact tables from these source system changes.\n",
        "    * **Performance:** Surrogate keys are typically integers (or a fixed-length hash like in our example), which are more efficient for joins in database systems compared to potentially long or composite string-based natural keys.\n",
        "    * **Decoupling:** Using surrogate keys decouples the data warehouse model from the operational source system's keying structure. This allows the source system to evolve its keys without breaking the warehouse.\n",
        "    * **Handling Slowly Changing Dimensions (SCDs):** Surrogate keys are essential for implementing SCD Type 2, where you need to track historical changes to dimension attributes. A new surrogate key is assigned for each version of a patient's record.\n",
        "    * **Integration of Multiple Sources:** If patients come from multiple source systems with different natural key formats or potential overlaps, a surrogate key provides a single, unambiguous way to identify a unique patient entity in the warehouse.\n",
        "\n",
        "2.  **How can Microsoft Purview (discussed in later sections) help in understanding the lineage of data as it moves from `Bronze_HL7_Encounters_Raw` to `Silver_Unified_Encounters` and finally into `Fact_Encounter`?**\n",
        "    * **Automated Lineage Tracking:** Microsoft Purview can scan Fabric items (Lakehouses, Notebooks, Data Factory pipelines) and automatically capture data lineage. It visualizes how data flows from source tables (e.g., `Bronze_HL7_Encounters_Raw`) through transformation processes (e.g., the Spark Notebooks that create `Silver_Unified_Encounters` and `Fact_Encounter`) to the final analytical tables and even into Power BI reports.\n",
        "    * **Impact Analysis:** If there's a change proposed to `Bronze_HL7_Encounters_Raw` or a transformation rule, lineage helps identify all downstream tables, reports, and processes (like `Silver_Unified_Encounters`, `Fact_Encounter`) that will be affected.\n",
        "    * **Troubleshooting & Root Cause Analysis:** If data quality issues are found in `Fact_Encounter`, lineage allows data stewards or engineers to trace back to the origin of the data and the transformations applied at each step, helping to pinpoint where the issue was introduced.\n",
        "    * **Compliance and Governance:** For regulations like HIPAA, demonstrating data provenance and how sensitive data elements are processed and transformed is critical. Purview's lineage provides this auditable trail.\n",
        "    * **Data Discovery:** Business users or analysts can use lineage to understand where the data in their reports or analytical models originates, increasing trust and understanding of the data.\n",
        "\n",
        "3.  **If a new field, like `DischargeDisposition`, needs to be added to the `Silver_Unified_Encounters` table later, how does using Delta Lake format for your Lakehouse tables make this process easier?**\n",
        "    * **Schema Evolution:** Delta Lake supports schema evolution, which means you can add new columns (like `DischargeDisposition`) to an existing Delta table without rewriting the entire table or breaking existing queries that don't use the new column.\n",
        "        * The Spark Notebook or Dataflow Gen2 performing the transformation to `Silver_Unified_Encounters` can be updated to include the new `DischargeDisposition` field.\n",
        "        * When this updated job runs and writes to the Delta table, the new column will be added to the table's schema automatically (if `mergeSchema` option is used or if the write mode supports it).\n",
        "    * **Schema Enforcement:** While allowing evolution, Delta Lake also offers schema enforcement. This prevents accidental schema changes due to bad data, but for intentional additions like `DischargeDisposition`, schema evolution handles it gracefully.\n",
        "    * **No Downtime for Readers:** Existing queries and downstream processes that read from `Silver_Unified_Encounters` but do not yet reference the new `DischargeDisposition` column will continue to work without modification. They will simply not see the new column until they are updated to use it.\n",
        "    * **Data Backfill:** After adding the column, historical records will have null values for `DischargeDisposition`. You can then run a separate job to backfill this new column for existing records if the data is available."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
